# Quantization and Racial Biases


Official repository for the "Compressed Models Decompress Race Biases: What Quantized Models Forget for Fair Face Recognition" paper  and winner of the best paper award at [BIOSIG 2023](https://biosig.de). 

The paper can be viewed at: [Arxiv](https://arxiv.org/abs/2308.11840) and [IEEE Xplore](https://ieeexplore.ieee.org/abstract/document/10346003)


<img src="visualization.png" width="600" align="right"> 

## Abstract

With the ever-growing complexity of deep learning models for face recognition, it becomes hard to deploy these systems in real life. Researchers have two options: 1) use smaller models; 2) compress their current models. Since the usage of smaller models might lead to concerning biases, compression gains relevance. However, compressing might be also responsible for an increase in the bias of the final model. We investigate the overall performance, the performance on each ethnicity subgroup and the racial bias of a State-of-the-Art quantization approach when used with synthetic and real data. This analysis provides a few more details on potential benefits of performing quantization with synthetic data, for instance, the reduction of biases on the majority of test scenarios. We tested five distinct architectures and three different training datasets. The models were evaluated on a fourth dataset which was collected to infer and compare the performance of face recognition models on different ethnicity.


## Datasets

This study utilized five distinct datasets. MS1MV2, BUPT-Balancedface and BUPT-Globalface have been used for training the base models and quantization, while the [synthetic data](https://github.com/fdbtrs/QuantFace) was only used for quantization and RFW just for evaluation of the models. 

### MS1MV2

MS1MV2 is widely used in the literature to train and compare several deep face recognition models. It is a refined version of the MS-Celeb-1M dataset, which further improved the training of these systems. The dataset contains 85k different identities and almost six million images and it is not balanced with respect to the race. 

### BUPT-Balancedface and BUPT-Globalface

Wang~\textit{et al.} introduced two distinct datasets to train deep face recognition systems. These datasets have been created to mitigate race bias on face recognition through skin tone labelling as African, Asian, Caucasian and Indian. BUPT-Globalface contains two million images from 38k different identities, and the distribution of races follows their distribution in the world. On the other hand, BUPT-Balancedface contains 1.3 million images from 28k identities which are divided into 7k identities per race. As such, this second dataset is race balanced. 


### Synthetic data

This dataset, introduced in contains approximately 500k unlabelled synthetic images. These images have been generated by a generative adversarial network. The noise used as input to generate the images was sampled from a Gaussian distribution and fed to a pretrained generator (official open source implementation of [StyleGAN2-AD](https://github.com/NVlabs/stylegan2-ada)). The usage of synthetic data is often seen to result in sub-optimal performances  which might be caused by a domain gap between real and synthetic data. In this work, the goal is not to use the synthetic data to learn the representations from scratch, and we further argue that there might exist advantages of this domain gap. 

### RFW

Racial Faces in-the-wild (RFW), was proposed by the same authors of BUPT-Balancedface, and was designed as a benchmarking dataset for fair face verification. Similarly, it includes labels for ethnicity, which allows for a fair assessment of potential biases. It contains 3000 individuals with 6000 image pairs for face verification.


## Results


A careful analysis of the performance of the different sized models at full precision **(Table 1)** shows that smaller models tend to have higher biases and lower performance in terms of average accuracy. ResNet-100 is an exception and this difference might be related to the fact that SER becomes highly sensitive when the errors are below 1\%.
<p align="center">
<img src="table_1.png" width="800" align="center"> 
</p>
The quantized version of these models seems to retain the performance and bias advantages when compared to simpler models. As theorised, the quantization has a negative impact on the bias, and in most cases on the performance too. The lower the number of bit, the higher the bias. However, the usage of synthetic data has shown, for all the different precisions, a capability to reduce the bias while retaining the performance. From this data, it is not clear if the improvement is due to a specific characteristic of the synthetic data. 

We have used the ethnicity classifier to get an estimation of the racial balance of the synthetic data. We obtained 365889 Caucasians, 81568 Asians, 81568 Indians and 61966 Africans. Since the data is not balanced, it is not possible to associate the effects of this data to its balance. 

<p align="center">
<img src="table_2.png" width="800" align="center"> 
</p>

Further training two ResNet-34 on BUPT-Balancedface and BUPT-Globalface shows, at full precision, that despite a higher performance of the latter, the balance of the former is essential to ensure better bias metrics. On the model trained with the BUPT-Balancedface the versions quantized with synthetic data has not only kept the same tendency of the previous table, but it has also surpassed by a large margin the version of the method quantized with the balanced data. This might be caused by the lower variability of the BUPT-Balancedface data with respect to the number of identities. 

The ResNet-34 trained on the BUPT-Globalface performs better if quantized with the data from the BUPT-Balancedface instead of using the data from training. Once again, it might be possible that introducing variability and unseen data for the quantization increases the capability of the model to be robust for all ethnicities. This is further validated by the version quantized with the synthetic data, which leads to a performance similar to the full precision model. 

## Citation 

```bibtex
@inproceedings{neto2023compressed,
  title={Compressed models decompress race biases: What quantized models forget for fair face recognition},
  author={Neto, Pedro C and Caldeira, Eduarda and Cardoso, Jaime S and Sequeira, Ana F},
  booktitle={2023 International Conference of the Biometrics Special Interest Group (BIOSIG)},
  pages={1--5},
  year={2023},
  organization={IEEE}
}
```

